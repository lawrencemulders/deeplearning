import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pprint

import tensorflow as tf
from tensorflow	import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras import layers
from tensorflow.keras.layers import InputLayer
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix

# Data Pre-Processing

dataset = pd.read_csv("cover_data.csv")

dataset = dataset.drop(['Aspect'], axis=1)

labels = dataset.iloc[:, -1]
features = dataset.iloc[:, :-1]

features_training_set, features_test_set, labels_training_set, labels_test_set = train_test_split(
    features,
    labels,
    test_size = 0.2,
    random_state = 42,
    stratify = labels
)

scaler = StandardScaler()

features_train_scaled = scaler.fit_transform(features_training_set)
features_test_scaled = scaler.fit_transform(features_test_set)

def design_model(learning_rate):

    model = Sequential()
    # Define the input layer
    model.add(InputLayer(input_shape = (features_train_scaled.shape[1],)))
    # Establish hidden layers
    model.add(Dense(128, activation = "relu"))
    model.add(Dense(64, activation = "relu"))
    model.add(Dense(32, activation = "relu"))
    model.add(Dense(16, activation = "relu"))
    model.add(Dense(8, activation = "relu"))
    # Establish the output layer
    model.add(Dense(8, activation = "softmax"))

    opt = Adam(learning_rate = learning_rate)
    model.compile(
        loss = "sparse_categorical_crossentropy",
        metrics = ["accuracy"],
        optimizer = opt
    )

    model.summary()
    return model

def fit_model(learning_rate, num_epochs, batchsize):
    model = design_model(learning_rate)

    stop = EarlyStopping(
        monitor = 'val_accuracy',
        mode = 'auto',
        verbose = 1,
        patience = 10
    )

    history = model.fit(
        features_train_scaled,
        labels_training_set,
        epochs = num_epochs,
        batch_size = batchsize,
        verbose = 1,
        validation_split = 0.1,
        callbacks = [stop]
    )
    return history, features_test_scaled, labels_test_set

def plot_accuracy(history):
    # Plot accuracy
    fig = plt.figure(figsize=(15, 10))
    ax1 = fig.add_subplot(2, 1, 1)
    ax1.plot(history.history['accuracy'])
    ax1.plot(history.history['val_accuracy'])
    ax1.set_title('model accuracy')
    ax1.set_ylabel('accuracy')
    ax1.set_xlabel('epoch')
    ax1.legend(['train', 'validation'], loc='upper left')

    # Plot loss and val_loss over each epoch
    ax2 = fig.add_subplot(2, 1, 2)
    ax2.plot(history.history['loss'])
    ax2.plot(history.history['val_loss'])
    ax2.set_title('plot learning curves')
    ax2.set_ylabel('loss')
    ax2.set_xlabel('epoch')
    ax2.legend(['train', 'validation'], loc='upper left')
    fig.tight_layout()

class_names = ['Spruce/Fir', 'Lodgepole Pine',
                   'Ponderosa Pine', 'Cottonwood/Willow',
                   'Aspen', 'Douglas-fir', 'Krummholz']

def report(model, features_test_scaled, labels_test_set):
    score = model.evaluate(features_test_scaled, labels_test_set, verbose = 0)
    print(f'Test loss: {score[0]}')
    print(f'Test accuracy: {score[1]}')

    # Evaluating the model:
    y_pred = model.predict(features_test_scaled)
    # Convert the prediction to discrete values
    y_pred = np.argmax(y_pred, axis=1)
    print(classification_report(labels_test_set, y_pred, target_names = class_names))
    plt.show()
    return y_pred

def plot_heatmap(class_names, y_pred, y_test):
    cm = confusion_matrix(y_test, y_pred)
    fig, ax = plt.subplots(figsize=(15, 15))
    heatmap = sns.heatmap(cm, fmt='g', cmap='Blues', annot=True, ax=ax)
    ax.set_xlabel('Predicted class')
    ax.set_ylabel('True class')
    ax.set_title('Confusion Matrix')
    ax.xaxis.set_ticklabels(class_names)
    ax.yaxis.set_ticklabels(class_names)
    # Save the heatmap to file
    heatmapfig = heatmap.get_figure()
    #heatmapfig.savefig(f'../output/confusion_matrix.png')
    plt.show()

learning_rate = 0.001
# Too many epochs can lead to overfitting, and too few to underfitting.
num_epochs = 200
batch_size = 1024

history, features_test_scaled, labels_test_set = fit_model(learning_rate, num_epochs, batch_size)

print(plot_accuracy(history))

y_pred = report(history.model, features_test_scaled, labels_test_set)

print(plot_heatmap(class_names, y_pred, labels_test_set))

###################################################################OUTPUT#################################################################################

Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense (Dense)               (None, 128)               6912      
                                                                 
 dense_1 (Dense)             (None, 64)                8256      
                                                                 
 dense_2 (Dense)             (None, 32)                2080      
                                                                 
 dense_3 (Dense)             (None, 16)                528       
                                                                 
 dense_4 (Dense)             (None, 8)                 136       
                                                                 
 dense_5 (Dense)             (None, 8)                 72        
                                                                 
=================================================================
Total params: 17,984
Trainable params: 17,984
Non-trainable params: 0
_________________________________________________________________

Epoch 1/200
409/409 [==============================] - 1s 2ms/step - loss: 0.8464 - accuracy: 0.6214 - val_loss: 0.6000 - val_accuracy: 0.7458
Epoch 2/200
409/409 [==============================] - 1s 2ms/step - loss: 0.5593 - accuracy: 0.7634 - val_loss: 0.5333 - val_accuracy: 0.7744
Epoch 3/200
409/409 [==============================] - 1s 2ms/step - loss: 0.5120 - accuracy: 0.7841 - val_loss: 0.5032 - val_accuracy: 0.7908
Epoch 4/200
409/409 [==============================] - 1s 2ms/step - loss: 0.4811 - accuracy: 0.8003 - val_loss: 0.4700 - val_accuracy: 0.8099
Epoch 5/200
409/409 [==============================] - 1s 2ms/step - loss: 0.4556 - accuracy: 0.8138 - val_loss: 0.4503 - val_accuracy: 0.8170
Epoch 6/200
409/409 [==============================] - 1s 2ms/step - loss: 0.4391 - accuracy: 0.8207 - val_loss: 0.4346 - val_accuracy: 0.8245
Epoch 7/200
409/409 [==============================] - 1s 2ms/step - loss: 0.4252 - accuracy: 0.8276 - val_loss: 0.4208 - val_accuracy: 0.8301
Epoch 8/200
409/409 [==============================] - 1s 2ms/step - loss: 0.4132 - accuracy: 0.8332 - val_loss: 0.4190 - val_accuracy: 0.8307
Epoch 9/200
409/409 [==============================] - 1s 2ms/step - loss: 0.4017 - accuracy: 0.8379 - val_loss: 0.4233 - val_accuracy: 0.8279
Epoch 10/200
409/409 [==============================] - 1s 2ms/step - loss: 0.3931 - accuracy: 0.8423 - val_loss: 0.3987 - val_accuracy: 0.8373
Epoch 11/200
409/409 [==============================] - 1s 2ms/step - loss: 0.3853 - accuracy: 0.8452 - val_loss: 0.3827 - val_accuracy: 0.8472
Epoch 12/200
409/409 [==============================] - 1s 2ms/step - loss: 0.3771 - accuracy: 0.8489 - val_loss: 0.3842 - val_accuracy: 0.8483
Epoch 13/200
409/409 [==============================] - 1s 2ms/step - loss: 0.3679 - accuracy: 0.8536 - val_loss: 0.3658 - val_accuracy: 0.8529
Epoch 14/200
409/409 [==============================] - 1s 2ms/step - loss: 0.3628 - accuracy: 0.8559 - val_loss: 0.3648 - val_accuracy: 0.8559
Epoch 15/200
409/409 [==============================] - 1s 2ms/step - loss: 0.3550 - accuracy: 0.8586 - val_loss: 0.3556 - val_accuracy: 0.8574
Epoch 16/200
409/409 [==============================] - 1s 2ms/step - loss: 0.3492 - accuracy: 0.8611 - val_loss: 0.3504 - val_accuracy: 0.8628
Epoch 17/200
409/409 [==============================] - 1s 2ms/step - loss: 0.3442 - accuracy: 0.8633 - val_loss: 0.3484 - val_accuracy: 0.8614
Epoch 18/200
409/409 [==============================] - 1s 2ms/step - loss: 0.3381 - accuracy: 0.8659 - val_loss: 0.3472 - val_accuracy: 0.8648
Epoch 19/200
409/409 [==============================] - 1s 2ms/step - loss: 0.3355 - accuracy: 0.8667 - val_loss: 0.3371 - val_accuracy: 0.8651
Epoch 20/200
409/409 [==============================] - 1s 2ms/step - loss: 0.3287 - accuracy: 0.8693 - val_loss: 0.3401 - val_accuracy: 0.8631
Epoch 21/200
409/409 [==============================] - 1s 2ms/step - loss: 0.3253 - accuracy: 0.8709 - val_loss: 0.3315 - val_accuracy: 0.8676
Epoch 22/200
409/409 [==============================] - 1s 2ms/step - loss: 0.3202 - accuracy: 0.8730 - val_loss: 0.3278 - val_accuracy: 0.8698
Epoch 23/200
409/409 [==============================] - 1s 2ms/step - loss: 0.3188 - accuracy: 0.8734 - val_loss: 0.3269 - val_accuracy: 0.8703
Epoch 24/200
409/409 [==============================] - 1s 2ms/step - loss: 0.3116 - accuracy: 0.8766 - val_loss: 0.3187 - val_accuracy: 0.8721
Epoch 25/200
409/409 [==============================] - 1s 2ms/step - loss: 0.3098 - accuracy: 0.8771 - val_loss: 0.3081 - val_accuracy: 0.8790
Epoch 26/200
409/409 [==============================] - 1s 2ms/step - loss: 0.3079 - accuracy: 0.8777 - val_loss: 0.3034 - val_accuracy: 0.8799
Epoch 27/200
409/409 [==============================] - 1s 2ms/step - loss: 0.3031 - accuracy: 0.8796 - val_loss: 0.2999 - val_accuracy: 0.8805
Epoch 28/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2991 - accuracy: 0.8813 - val_loss: 0.3001 - val_accuracy: 0.8805
Epoch 29/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2963 - accuracy: 0.8820 - val_loss: 0.3082 - val_accuracy: 0.8758
Epoch 30/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2948 - accuracy: 0.8825 - val_loss: 0.3035 - val_accuracy: 0.8776
Epoch 31/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2920 - accuracy: 0.8835 - val_loss: 0.3056 - val_accuracy: 0.8782
Epoch 32/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2901 - accuracy: 0.8840 - val_loss: 0.3105 - val_accuracy: 0.8753
Epoch 33/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2865 - accuracy: 0.8853 - val_loss: 0.2876 - val_accuracy: 0.8871
Epoch 34/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2851 - accuracy: 0.8863 - val_loss: 0.2846 - val_accuracy: 0.8866
Epoch 35/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2828 - accuracy: 0.8870 - val_loss: 0.3010 - val_accuracy: 0.8793
Epoch 36/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2796 - accuracy: 0.8882 - val_loss: 0.2791 - val_accuracy: 0.8898
Epoch 37/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2783 - accuracy: 0.8890 - val_loss: 0.2780 - val_accuracy: 0.8895
Epoch 38/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2755 - accuracy: 0.8903 - val_loss: 0.2802 - val_accuracy: 0.8898
Epoch 39/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2754 - accuracy: 0.8908 - val_loss: 0.2767 - val_accuracy: 0.8894
Epoch 40/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2728 - accuracy: 0.8913 - val_loss: 0.2808 - val_accuracy: 0.8886
Epoch 41/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2687 - accuracy: 0.8930 - val_loss: 0.2766 - val_accuracy: 0.8926
Epoch 42/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2697 - accuracy: 0.8926 - val_loss: 0.2758 - val_accuracy: 0.8910
Epoch 43/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2691 - accuracy: 0.8926 - val_loss: 0.2702 - val_accuracy: 0.8924
Epoch 44/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2634 - accuracy: 0.8953 - val_loss: 0.2714 - val_accuracy: 0.8930
Epoch 45/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2640 - accuracy: 0.8946 - val_loss: 0.2696 - val_accuracy: 0.8933
Epoch 46/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2625 - accuracy: 0.8954 - val_loss: 0.2815 - val_accuracy: 0.8893
Epoch 47/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2609 - accuracy: 0.8957 - val_loss: 0.2672 - val_accuracy: 0.8945
Epoch 48/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2581 - accuracy: 0.8974 - val_loss: 0.2609 - val_accuracy: 0.8980
Epoch 49/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2610 - accuracy: 0.8958 - val_loss: 0.2702 - val_accuracy: 0.8935
Epoch 50/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2552 - accuracy: 0.8986 - val_loss: 0.2708 - val_accuracy: 0.8914
Epoch 51/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2574 - accuracy: 0.8977 - val_loss: 0.2652 - val_accuracy: 0.8952
Epoch 52/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2546 - accuracy: 0.8983 - val_loss: 0.2588 - val_accuracy: 0.8980
Epoch 53/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2525 - accuracy: 0.8997 - val_loss: 0.2637 - val_accuracy: 0.8942
Epoch 54/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2511 - accuracy: 0.8996 - val_loss: 0.2617 - val_accuracy: 0.8959
Epoch 55/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2500 - accuracy: 0.9004 - val_loss: 0.2654 - val_accuracy: 0.8938
Epoch 56/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2496 - accuracy: 0.9001 - val_loss: 0.2519 - val_accuracy: 0.9007
Epoch 57/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2469 - accuracy: 0.9018 - val_loss: 0.2580 - val_accuracy: 0.8979
Epoch 58/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2459 - accuracy: 0.9018 - val_loss: 0.2533 - val_accuracy: 0.8990
Epoch 59/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2464 - accuracy: 0.9017 - val_loss: 0.2467 - val_accuracy: 0.9018
Epoch 60/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2440 - accuracy: 0.9029 - val_loss: 0.2506 - val_accuracy: 0.9008
Epoch 61/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2438 - accuracy: 0.9025 - val_loss: 0.2665 - val_accuracy: 0.8952
Epoch 62/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2410 - accuracy: 0.9038 - val_loss: 0.2656 - val_accuracy: 0.8935
Epoch 63/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2418 - accuracy: 0.9037 - val_loss: 0.2523 - val_accuracy: 0.9006
Epoch 64/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2415 - accuracy: 0.9038 - val_loss: 0.2461 - val_accuracy: 0.9022
Epoch 65/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2387 - accuracy: 0.9050 - val_loss: 0.2439 - val_accuracy: 0.9036
Epoch 66/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2390 - accuracy: 0.9043 - val_loss: 0.2446 - val_accuracy: 0.9037
Epoch 67/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2385 - accuracy: 0.9042 - val_loss: 0.2451 - val_accuracy: 0.9035
Epoch 68/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2360 - accuracy: 0.9056 - val_loss: 0.2414 - val_accuracy: 0.9050
Epoch 69/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2340 - accuracy: 0.9067 - val_loss: 0.2690 - val_accuracy: 0.8940
Epoch 70/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2358 - accuracy: 0.9059 - val_loss: 0.2379 - val_accuracy: 0.9059
Epoch 71/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2337 - accuracy: 0.9065 - val_loss: 0.2440 - val_accuracy: 0.9036
Epoch 72/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2319 - accuracy: 0.9074 - val_loss: 0.2351 - val_accuracy: 0.9080
Epoch 73/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2324 - accuracy: 0.9070 - val_loss: 0.2376 - val_accuracy: 0.9070
Epoch 74/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2310 - accuracy: 0.9077 - val_loss: 0.2399 - val_accuracy: 0.9075
Epoch 75/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2311 - accuracy: 0.9076 - val_loss: 0.2350 - val_accuracy: 0.9088
Epoch 76/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2271 - accuracy: 0.9093 - val_loss: 0.2399 - val_accuracy: 0.9062
Epoch 77/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2289 - accuracy: 0.9088 - val_loss: 0.2425 - val_accuracy: 0.9029
Epoch 78/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2272 - accuracy: 0.9097 - val_loss: 0.2429 - val_accuracy: 0.9028
Epoch 79/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2281 - accuracy: 0.9087 - val_loss: 0.2455 - val_accuracy: 0.9024
Epoch 80/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2261 - accuracy: 0.9100 - val_loss: 0.2344 - val_accuracy: 0.9088
Epoch 81/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2269 - accuracy: 0.9095 - val_loss: 0.2369 - val_accuracy: 0.9073
Epoch 82/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2250 - accuracy: 0.9101 - val_loss: 0.2320 - val_accuracy: 0.9093
Epoch 83/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2237 - accuracy: 0.9108 - val_loss: 0.2308 - val_accuracy: 0.9101
Epoch 84/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2224 - accuracy: 0.9114 - val_loss: 0.2329 - val_accuracy: 0.9077
Epoch 85/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2231 - accuracy: 0.9108 - val_loss: 0.2413 - val_accuracy: 0.9038
Epoch 86/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2204 - accuracy: 0.9120 - val_loss: 0.2338 - val_accuracy: 0.9071
Epoch 87/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2205 - accuracy: 0.9119 - val_loss: 0.2298 - val_accuracy: 0.9093
Epoch 88/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2222 - accuracy: 0.9110 - val_loss: 0.2565 - val_accuracy: 0.8961
Epoch 89/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2208 - accuracy: 0.9118 - val_loss: 0.2322 - val_accuracy: 0.9100
Epoch 90/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2208 - accuracy: 0.9116 - val_loss: 0.2403 - val_accuracy: 0.9060
Epoch 91/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2180 - accuracy: 0.9132 - val_loss: 0.2259 - val_accuracy: 0.9108
Epoch 92/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2172 - accuracy: 0.9134 - val_loss: 0.2301 - val_accuracy: 0.9086
Epoch 93/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2213 - accuracy: 0.9117 - val_loss: 0.2577 - val_accuracy: 0.8945
Epoch 94/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2168 - accuracy: 0.9131 - val_loss: 0.2233 - val_accuracy: 0.9133
Epoch 95/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2164 - accuracy: 0.9134 - val_loss: 0.2387 - val_accuracy: 0.9045
Epoch 96/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2142 - accuracy: 0.9149 - val_loss: 0.2373 - val_accuracy: 0.9054
Epoch 97/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2150 - accuracy: 0.9139 - val_loss: 0.2307 - val_accuracy: 0.9091
Epoch 98/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2159 - accuracy: 0.9133 - val_loss: 0.2261 - val_accuracy: 0.9105
Epoch 99/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2131 - accuracy: 0.9148 - val_loss: 0.2352 - val_accuracy: 0.9081
Epoch 100/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2126 - accuracy: 0.9155 - val_loss: 0.2231 - val_accuracy: 0.9138
Epoch 101/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2143 - accuracy: 0.9148 - val_loss: 0.2194 - val_accuracy: 0.9138
Epoch 102/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2124 - accuracy: 0.9153 - val_loss: 0.2347 - val_accuracy: 0.9068
Epoch 103/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2107 - accuracy: 0.9162 - val_loss: 0.2235 - val_accuracy: 0.9112
Epoch 104/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2110 - accuracy: 0.9161 - val_loss: 0.2222 - val_accuracy: 0.9114
Epoch 105/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2102 - accuracy: 0.9157 - val_loss: 0.2241 - val_accuracy: 0.9124
Epoch 106/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2098 - accuracy: 0.9161 - val_loss: 0.2283 - val_accuracy: 0.9097
Epoch 107/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2098 - accuracy: 0.9162 - val_loss: 0.2185 - val_accuracy: 0.9154
Epoch 108/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2078 - accuracy: 0.9171 - val_loss: 0.2175 - val_accuracy: 0.9130
Epoch 109/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2082 - accuracy: 0.9173 - val_loss: 0.2267 - val_accuracy: 0.9112
Epoch 110/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2078 - accuracy: 0.9170 - val_loss: 0.2213 - val_accuracy: 0.9120
Epoch 111/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2083 - accuracy: 0.9170 - val_loss: 0.2290 - val_accuracy: 0.9086
Epoch 112/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2066 - accuracy: 0.9175 - val_loss: 0.2426 - val_accuracy: 0.9035
Epoch 113/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2077 - accuracy: 0.9171 - val_loss: 0.2229 - val_accuracy: 0.9126
Epoch 114/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2061 - accuracy: 0.9177 - val_loss: 0.2168 - val_accuracy: 0.9148
Epoch 115/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2042 - accuracy: 0.9186 - val_loss: 0.2126 - val_accuracy: 0.9173
Epoch 116/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2057 - accuracy: 0.9181 - val_loss: 0.2280 - val_accuracy: 0.9105
Epoch 117/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2039 - accuracy: 0.9186 - val_loss: 0.2223 - val_accuracy: 0.9133
Epoch 118/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2054 - accuracy: 0.9180 - val_loss: 0.2182 - val_accuracy: 0.9142
Epoch 119/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2032 - accuracy: 0.9188 - val_loss: 0.2263 - val_accuracy: 0.9099
Epoch 120/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2030 - accuracy: 0.9192 - val_loss: 0.2241 - val_accuracy: 0.9105
Epoch 121/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2039 - accuracy: 0.9186 - val_loss: 0.2191 - val_accuracy: 0.9155
Epoch 122/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2026 - accuracy: 0.9192 - val_loss: 0.2197 - val_accuracy: 0.9131
Epoch 123/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2025 - accuracy: 0.9187 - val_loss: 0.2105 - val_accuracy: 0.9176
Epoch 124/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2026 - accuracy: 0.9190 - val_loss: 0.2167 - val_accuracy: 0.9151
Epoch 125/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2027 - accuracy: 0.9192 - val_loss: 0.2135 - val_accuracy: 0.9162
Epoch 126/200
409/409 [==============================] - 1s 2ms/step - loss: 0.1987 - accuracy: 0.9209 - val_loss: 0.2176 - val_accuracy: 0.9134
Epoch 127/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2013 - accuracy: 0.9197 - val_loss: 0.2144 - val_accuracy: 0.9169
Epoch 128/200
409/409 [==============================] - 1s 2ms/step - loss: 0.1994 - accuracy: 0.9203 - val_loss: 0.2144 - val_accuracy: 0.9166
Epoch 129/200
409/409 [==============================] - 1s 2ms/step - loss: 0.1999 - accuracy: 0.9202 - val_loss: 0.2186 - val_accuracy: 0.9146
Epoch 130/200
409/409 [==============================] - 1s 2ms/step - loss: 0.1986 - accuracy: 0.9208 - val_loss: 0.2591 - val_accuracy: 0.8961
Epoch 131/200
409/409 [==============================] - 1s 2ms/step - loss: 0.1997 - accuracy: 0.9203 - val_loss: 0.2238 - val_accuracy: 0.9108
Epoch 132/200
409/409 [==============================] - 1s 2ms/step - loss: 0.1981 - accuracy: 0.9208 - val_loss: 0.2170 - val_accuracy: 0.9155
Epoch 133/200
409/409 [==============================] - 1s 2ms/step - loss: 0.1983 - accuracy: 0.9206 - val_loss: 0.2128 - val_accuracy: 0.9166
Epoch 133: early stopping
None
Test loss: 0.21938973665237427
Test accuracy: 0.9137285351753235
3632/3632 [==============================] - 1s 264us/step
                   precision    recall  f1-score   support

       Spruce/Fir       0.91      0.92      0.91     42368
   Lodgepole Pine       0.93      0.92      0.93     56661
   Ponderosa Pine       0.88      0.95      0.91      7151
Cottonwood/Willow       0.84      0.80      0.82       549
            Aspen       0.82      0.71      0.76      1899
      Douglas-fir       0.89      0.78      0.83      3473
        Krummholz       0.93      0.93      0.93      4102

         accuracy                           0.91    116203
        macro avg       0.88      0.86      0.87    116203
     weighted avg       0.91      0.91      0.91    116203
     
