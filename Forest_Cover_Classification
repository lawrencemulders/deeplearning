import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pprint

import tensorflow as tf
from tensorflow	import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras import layers
from tensorflow.keras.layers import InputLayer
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix

# Data Pre-Processing

dataset = pd.read_csv("cover_data.csv")

dataset = dataset.drop(['Aspect'], axis=1)

labels = dataset.iloc[:, -1]
features = dataset.iloc[:, :-1]

features_training_set, features_test_set, labels_training_set, labels_test_set = train_test_split(
    features,
    labels,
    test_size = 0.2,
    random_state = 42,
    stratify = labels
)

scaler = StandardScaler()

features_train_scaled = scaler.fit_transform(features_training_set)
features_test_scaled = scaler.fit_transform(features_test_set)

def design_model(learning_rate):

    model = Sequential()
    # Define the input layer
    model.add(InputLayer(input_shape = (features_train_scaled.shape[1],)))
    # Establish hidden layers
    model.add(Dense(128, activation = "relu"))
    model.add(Dense(64, activation = "relu"))
    model.add(Dense(32, activation = "relu"))
    model.add(Dense(16, activation = "relu"))
    model.add(Dense(8, activation = "relu"))
    # Establish the output layer
    model.add(Dense(8, activation = "softmax"))

    opt = Adam(learning_rate = learning_rate)
    model.compile(
        loss = "sparse_categorical_crossentropy",
        metrics = ["accuracy"],
        optimizer = opt
    )

    model.summary()
    return model

def fit_model(learning_rate, num_epochs, batchsize):
    model = design_model(learning_rate)

    stop = EarlyStopping(
        monitor = 'val_accuracy',
        mode = 'auto',
        verbose = 1,
        patience = 10
    )

    history = model.fit(
        features_train_scaled,
        labels_training_set,
        epochs = num_epochs,
        batch_size = batchsize,
        verbose = 1,
        validation_split = 0.1,
        callbacks = [stop]
    )
    return history, features_test_scaled, labels_test_set

def plot_accuracy(history):
    # Plot accuracy
    fig = plt.figure(figsize=(15, 10))
    ax1 = fig.add_subplot(2, 1, 1)
    ax1.plot(history.history['accuracy'])
    ax1.plot(history.history['val_accuracy'])
    ax1.set_title('model accuracy')
    ax1.set_ylabel('accuracy')
    ax1.set_xlabel('epoch')
    ax1.legend(['train', 'validation'], loc='upper left')

    # Plot loss and val_loss over each epoch
    ax2 = fig.add_subplot(2, 1, 2)
    ax2.plot(history.history['loss'])
    ax2.plot(history.history['val_loss'])
    ax2.set_title('plot learning curves')
    ax2.set_ylabel('loss')
    ax2.set_xlabel('epoch')
    ax2.legend(['train', 'validation'], loc='upper left')
    fig.tight_layout()

class_names = ['Spruce/Fir', 'Lodgepole Pine',
                   'Ponderosa Pine', 'Cottonwood/Willow',
                   'Aspen', 'Douglas-fir', 'Krummholz']

def report(model, features_test_scaled, labels_test_set):
    score = model.evaluate(features_test_scaled, labels_test_set, verbose = 0)
    print(f'Test loss: {score[0]}')
    print(f'Test accuracy: {score[1]}')

    # Evaluating the model:
    y_pred = model.predict(features_test_scaled)
    # Convert the prediction to discrete values
    y_pred = np.argmax(y_pred, axis=1)
    print(classification_report(labels_test_set, y_pred, target_names = class_names))
    plt.show()
    return y_pred

def plot_heatmap(class_names, y_pred, y_test):
    cm = confusion_matrix(y_test, y_pred)
    fig, ax = plt.subplots(figsize=(15, 15))
    heatmap = sns.heatmap(cm, fmt='g', cmap='Blues', annot=True, ax=ax)
    ax.set_xlabel('Predicted class')
    ax.set_ylabel('True class')
    ax.set_title('Confusion Matrix')
    ax.xaxis.set_ticklabels(class_names)
    ax.yaxis.set_ticklabels(class_names)
    # Save the heatmap to file
    heatmapfig = heatmap.get_figure()
    #heatmapfig.savefig(f'../output/confusion_matrix.png')
    plt.show()

learning_rate = 0.001
# Too many epochs can lead to overfitting, and too few to underfitting.
num_epochs = 200
batch_size = 1024

history, features_test_scaled, labels_test_set = fit_model(learning_rate, num_epochs, batch_size)

print(plot_accuracy(history))

y_pred = report(history.model, features_test_scaled, labels_test_set)

print(plot_heatmap(class_names, y_pred, labels_test_set))

/Users/lawrencemulders/miniconda3/envs/my_tensorflowdeeplearning/bin/python /Users/lawrencemulders/PycharmProjects/deeplearning_forest_cover_classification/model.py 
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense (Dense)               (None, 128)               6912      
                                                                 
 dense_1 (Dense)             (None, 64)                8256      
                                                                 
 dense_2 (Dense)             (None, 32)                2080      
                                                                 
 dense_3 (Dense)             (None, 16)                528       
                                                                 
 dense_4 (Dense)             (None, 8)                 136       
                                                                 
 dense_5 (Dense)             (None, 8)                 72        
                                                                 
=================================================================
Total params: 17,984
Trainable params: 17,984
Non-trainable params: 0
_________________________________________________________________
/Users/lawrencemulders/miniconda3/envs/my_tensorflowdeeplearning/lib/python3.10/site-packages/keras/engine/data_adapter.py:1696: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.
  return t[start:end]
2023-02-05 14:45:02.024412: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz
Epoch 1/200
409/409 [==============================] - 1s 2ms/step - loss: 0.7654 - accuracy: 0.7082 - val_loss: 0.5791 - val_accuracy: 0.7605
Epoch 2/200
409/409 [==============================] - 1s 2ms/step - loss: 0.5443 - accuracy: 0.7725 - val_loss: 0.5275 - val_accuracy: 0.7737
Epoch 3/200
409/409 [==============================] - 1s 2ms/step - loss: 0.4946 - accuracy: 0.7910 - val_loss: 0.4791 - val_accuracy: 0.7980
Epoch 4/200
409/409 [==============================] - 1s 2ms/step - loss: 0.4640 - accuracy: 0.8056 - val_loss: 0.4719 - val_accuracy: 0.8022
Epoch 5/200
409/409 [==============================] - 1s 2ms/step - loss: 0.4405 - accuracy: 0.8163 - val_loss: 0.4385 - val_accuracy: 0.8171
Epoch 6/200
409/409 [==============================] - 1s 2ms/step - loss: 0.4231 - accuracy: 0.8242 - val_loss: 0.4223 - val_accuracy: 0.8260
Epoch 7/200
409/409 [==============================] - 1s 2ms/step - loss: 0.4087 - accuracy: 0.8307 - val_loss: 0.4011 - val_accuracy: 0.8356
Epoch 8/200
409/409 [==============================] - 1s 2ms/step - loss: 0.3947 - accuracy: 0.8380 - val_loss: 0.3896 - val_accuracy: 0.8425
Epoch 9/200
409/409 [==============================] - 1s 2ms/step - loss: 0.3841 - accuracy: 0.8430 - val_loss: 0.3800 - val_accuracy: 0.8459
Epoch 10/200
409/409 [==============================] - 1s 2ms/step - loss: 0.3734 - accuracy: 0.8483 - val_loss: 0.3706 - val_accuracy: 0.8499
Epoch 11/200
409/409 [==============================] - 1s 2ms/step - loss: 0.3678 - accuracy: 0.8506 - val_loss: 0.3632 - val_accuracy: 0.8536
Epoch 12/200
409/409 [==============================] - 1s 2ms/step - loss: 0.3583 - accuracy: 0.8550 - val_loss: 0.3610 - val_accuracy: 0.8542
Epoch 13/200
409/409 [==============================] - 1s 2ms/step - loss: 0.3531 - accuracy: 0.8567 - val_loss: 0.3483 - val_accuracy: 0.8606
Epoch 14/200
409/409 [==============================] - 1s 2ms/step - loss: 0.3448 - accuracy: 0.8611 - val_loss: 0.3444 - val_accuracy: 0.8603
Epoch 15/200
409/409 [==============================] - 1s 2ms/step - loss: 0.3396 - accuracy: 0.8635 - val_loss: 0.3408 - val_accuracy: 0.8641
Epoch 16/200
409/409 [==============================] - 1s 2ms/step - loss: 0.3354 - accuracy: 0.8647 - val_loss: 0.3581 - val_accuracy: 0.8521
Epoch 17/200
409/409 [==============================] - 1s 2ms/step - loss: 0.3314 - accuracy: 0.8664 - val_loss: 0.3348 - val_accuracy: 0.8638
Epoch 18/200
409/409 [==============================] - 1s 2ms/step - loss: 0.3262 - accuracy: 0.8681 - val_loss: 0.3336 - val_accuracy: 0.8661
Epoch 19/200
409/409 [==============================] - 1s 2ms/step - loss: 0.3219 - accuracy: 0.8702 - val_loss: 0.3268 - val_accuracy: 0.8688
Epoch 20/200
409/409 [==============================] - 1s 2ms/step - loss: 0.3176 - accuracy: 0.8722 - val_loss: 0.3264 - val_accuracy: 0.8677
Epoch 21/200
409/409 [==============================] - 1s 2ms/step - loss: 0.3139 - accuracy: 0.8738 - val_loss: 0.3140 - val_accuracy: 0.8755
Epoch 22/200
409/409 [==============================] - 1s 2ms/step - loss: 0.3123 - accuracy: 0.8741 - val_loss: 0.3124 - val_accuracy: 0.8740
Epoch 23/200
409/409 [==============================] - 1s 2ms/step - loss: 0.3068 - accuracy: 0.8766 - val_loss: 0.3191 - val_accuracy: 0.8702
Epoch 24/200
409/409 [==============================] - 1s 2ms/step - loss: 0.3035 - accuracy: 0.8777 - val_loss: 0.3125 - val_accuracy: 0.8721
Epoch 25/200
409/409 [==============================] - 1s 2ms/step - loss: 0.3023 - accuracy: 0.8779 - val_loss: 0.3114 - val_accuracy: 0.8734
Epoch 26/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2981 - accuracy: 0.8800 - val_loss: 0.3001 - val_accuracy: 0.8808
Epoch 27/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2959 - accuracy: 0.8807 - val_loss: 0.3029 - val_accuracy: 0.8794
Epoch 28/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2927 - accuracy: 0.8825 - val_loss: 0.3008 - val_accuracy: 0.8790
Epoch 29/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2907 - accuracy: 0.8832 - val_loss: 0.2965 - val_accuracy: 0.8824
Epoch 30/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2899 - accuracy: 0.8835 - val_loss: 0.2910 - val_accuracy: 0.8823
Epoch 31/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2869 - accuracy: 0.8849 - val_loss: 0.2931 - val_accuracy: 0.8810
Epoch 32/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2844 - accuracy: 0.8855 - val_loss: 0.2896 - val_accuracy: 0.8837
Epoch 33/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2818 - accuracy: 0.8870 - val_loss: 0.2862 - val_accuracy: 0.8852
Epoch 34/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2816 - accuracy: 0.8866 - val_loss: 0.3087 - val_accuracy: 0.8708
Epoch 35/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2796 - accuracy: 0.8875 - val_loss: 0.2848 - val_accuracy: 0.8837
Epoch 36/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2758 - accuracy: 0.8891 - val_loss: 0.3029 - val_accuracy: 0.8763
Epoch 37/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2768 - accuracy: 0.8881 - val_loss: 0.2807 - val_accuracy: 0.8875
Epoch 38/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2729 - accuracy: 0.8899 - val_loss: 0.2781 - val_accuracy: 0.8877
Epoch 39/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2721 - accuracy: 0.8904 - val_loss: 0.2916 - val_accuracy: 0.8799
Epoch 40/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2703 - accuracy: 0.8910 - val_loss: 0.2864 - val_accuracy: 0.8824
Epoch 41/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2709 - accuracy: 0.8913 - val_loss: 0.2705 - val_accuracy: 0.8900
Epoch 42/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2663 - accuracy: 0.8929 - val_loss: 0.2883 - val_accuracy: 0.8828
Epoch 43/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2665 - accuracy: 0.8930 - val_loss: 0.2786 - val_accuracy: 0.8860
Epoch 44/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2628 - accuracy: 0.8937 - val_loss: 0.2744 - val_accuracy: 0.8897
Epoch 45/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2640 - accuracy: 0.8938 - val_loss: 0.2702 - val_accuracy: 0.8904
Epoch 46/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2619 - accuracy: 0.8947 - val_loss: 0.2794 - val_accuracy: 0.8873
Epoch 47/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2629 - accuracy: 0.8946 - val_loss: 0.2687 - val_accuracy: 0.8926
Epoch 48/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2592 - accuracy: 0.8959 - val_loss: 0.2706 - val_accuracy: 0.8910
Epoch 49/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2589 - accuracy: 0.8959 - val_loss: 0.2808 - val_accuracy: 0.8865
Epoch 50/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2583 - accuracy: 0.8962 - val_loss: 0.2783 - val_accuracy: 0.8864
Epoch 51/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2585 - accuracy: 0.8963 - val_loss: 0.2639 - val_accuracy: 0.8933
Epoch 52/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2536 - accuracy: 0.8982 - val_loss: 0.2684 - val_accuracy: 0.8908
Epoch 53/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2551 - accuracy: 0.8971 - val_loss: 0.2631 - val_accuracy: 0.8938
Epoch 54/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2532 - accuracy: 0.8981 - val_loss: 0.2765 - val_accuracy: 0.8884
Epoch 55/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2515 - accuracy: 0.8990 - val_loss: 0.2723 - val_accuracy: 0.8900
Epoch 56/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2501 - accuracy: 0.8999 - val_loss: 0.2551 - val_accuracy: 0.8978
Epoch 57/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2497 - accuracy: 0.9000 - val_loss: 0.2627 - val_accuracy: 0.8934
Epoch 58/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2510 - accuracy: 0.8991 - val_loss: 0.2635 - val_accuracy: 0.8925
Epoch 59/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2481 - accuracy: 0.9007 - val_loss: 0.2668 - val_accuracy: 0.8918
Epoch 60/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2478 - accuracy: 0.9000 - val_loss: 0.2532 - val_accuracy: 0.8983
Epoch 61/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2450 - accuracy: 0.9018 - val_loss: 0.2541 - val_accuracy: 0.8984
Epoch 62/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2443 - accuracy: 0.9020 - val_loss: 0.2543 - val_accuracy: 0.8985
Epoch 63/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2448 - accuracy: 0.9017 - val_loss: 0.2676 - val_accuracy: 0.8910
Epoch 64/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2429 - accuracy: 0.9027 - val_loss: 0.2595 - val_accuracy: 0.8963
Epoch 65/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2416 - accuracy: 0.9031 - val_loss: 0.2549 - val_accuracy: 0.8977
Epoch 66/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2424 - accuracy: 0.9027 - val_loss: 0.2596 - val_accuracy: 0.8943
Epoch 67/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2406 - accuracy: 0.9036 - val_loss: 0.2498 - val_accuracy: 0.8995
Epoch 68/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2415 - accuracy: 0.9027 - val_loss: 0.2477 - val_accuracy: 0.8993
Epoch 69/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2380 - accuracy: 0.9043 - val_loss: 0.2473 - val_accuracy: 0.9006
Epoch 70/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2372 - accuracy: 0.9047 - val_loss: 0.2659 - val_accuracy: 0.8945
Epoch 71/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2404 - accuracy: 0.9032 - val_loss: 0.2496 - val_accuracy: 0.9004
Epoch 72/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2383 - accuracy: 0.9043 - val_loss: 0.2371 - val_accuracy: 0.9054
Epoch 73/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2344 - accuracy: 0.9060 - val_loss: 0.2500 - val_accuracy: 0.8999
Epoch 74/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2354 - accuracy: 0.9054 - val_loss: 0.2463 - val_accuracy: 0.9002
Epoch 75/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2347 - accuracy: 0.9057 - val_loss: 0.2482 - val_accuracy: 0.8992
Epoch 76/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2342 - accuracy: 0.9062 - val_loss: 0.2465 - val_accuracy: 0.9003
Epoch 77/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2326 - accuracy: 0.9069 - val_loss: 0.2414 - val_accuracy: 0.9025
Epoch 78/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2326 - accuracy: 0.9066 - val_loss: 0.2359 - val_accuracy: 0.9062
Epoch 79/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2329 - accuracy: 0.9060 - val_loss: 0.2395 - val_accuracy: 0.9050
Epoch 80/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2331 - accuracy: 0.9060 - val_loss: 0.2467 - val_accuracy: 0.9002
Epoch 81/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2295 - accuracy: 0.9074 - val_loss: 0.2428 - val_accuracy: 0.9031
Epoch 82/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2289 - accuracy: 0.9084 - val_loss: 0.2461 - val_accuracy: 0.9017
Epoch 83/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2307 - accuracy: 0.9069 - val_loss: 0.2410 - val_accuracy: 0.9052
Epoch 84/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2276 - accuracy: 0.9090 - val_loss: 0.2398 - val_accuracy: 0.9028
Epoch 85/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2287 - accuracy: 0.9081 - val_loss: 0.2374 - val_accuracy: 0.9060
Epoch 86/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2283 - accuracy: 0.9085 - val_loss: 0.2336 - val_accuracy: 0.9059
Epoch 87/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2257 - accuracy: 0.9099 - val_loss: 0.2446 - val_accuracy: 0.8999
Epoch 88/200
409/409 [==============================] - 1s 2ms/step - loss: 0.2263 - accuracy: 0.9090 - val_loss: 0.2402 - val_accuracy: 0.9055
Epoch 88: early stopping
None
Test loss: 0.2442665547132492
Test accuracy: 0.903298556804657
3632/3632 [==============================] - 1s 264us/step
                   precision    recall  f1-score   support

       Spruce/Fir       0.93      0.88      0.90     42368
   Lodgepole Pine       0.91      0.93      0.92     56661
   Ponderosa Pine       0.84      0.90      0.87      7151
Cottonwood/Willow       0.79      0.81      0.80       549
            Aspen       0.75      0.70      0.72      1899
      Douglas-fir       0.76      0.78      0.77      3473
        Krummholz       0.92      0.94      0.93      4102

         accuracy                           0.90    116203
        macro avg       0.84      0.85      0.85    116203
     weighted avg       0.90      0.90      0.90    116203

